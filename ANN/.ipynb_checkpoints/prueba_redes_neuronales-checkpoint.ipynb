{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f3eab3263df7bff172a5c0391ba4e45c",
     "grade": false,
     "grade_id": "cell-d0bdb64a4a5e1058",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# INFO257 Inteligencia Artificial: Prueba 3\n",
    "\n",
    "- Profesor responsable: Pablo Huijse\n",
    "- Consultas: phuijse at inf dot uach dot cl o slack\n",
    "\n",
    "## Instrucciones generales\n",
    "\n",
    "- No elimine ni agregue bloques a este cuadernillo\n",
    "- Conteste en los espacios indicados. Puede usar texto plano, markdown o latex. También puede insertar imágenes (menú View/Insert Image)\n",
    "- La prueba es individual: Está prohibido discutir la prueba con sus compañeros\n",
    "- Siga el [código de ética de la ACM](https://www.acm.org/about-acm/code-of-ethics-in-spanish): Sea honesto y leal\n",
    "- Entregue su trabajo por correo antes de la hora de término\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2d01f12f8e883d87151db46dd574989",
     "grade": false,
     "grade_id": "cell-d5dc446765cbc1dc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Pregunta 0\n",
    "\n",
    "Escriba su nombre completo en el siguiente bloque:\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f91554b2ef42f6145e893af53c748089",
     "grade": true,
     "grade_id": "cell-ef31b70a552b5868",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "FELIPE ANDRES SALDIAS TEILLIER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7546d45fa9705d406b524ae922b67812",
     "grade": false,
     "grade_id": "cell-b7c86b84828409b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Pregunta 1\n",
    "\n",
    "Sea una función de costo continua y derivable $L(\\theta)$ sobre un modelo con un parámetro $\\theta$\n",
    "\n",
    "Podemos buscar un mínimo de $L$ usando\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} =  \\theta_t - \\eta \\frac{dL}{d\\theta},\n",
    "$$\n",
    "\n",
    "que se conoce como la regla iterativa de gradiente descendente (GD)\n",
    "\n",
    "- (2pt) Explique conceptualmente como funciona la regla de GD y el significado de cada término de la ecuación\n",
    "- (1pt) Explique que ocurre si el valor de $\\eta$ es demasiado pequeño o demasiado grande\n",
    "- (1pt) Explique que ocurre si $L(\\theta)$ tiene mínimos locales. Explique dos estrategias que faciliten la búsqueda del óptimo global en este caso\n",
    "- (2pt) Indique las diferencias entre GD de tipo estocástico y batch. Explicite ventajas y desventajas de cada uno. Contraste con GD usando mini-batch\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ead10f47f57fb97f27cc63f56200f31",
     "grade": true,
     "grade_id": "cell-f2ce7ca77d552490",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. El gradiente descendente define una manera iterativa para intentar encontrar un optimo dentro de una funcion de coste. El objetivo siempre es ir disminuyendo el valor total de esta funcion objetivo la tipicamente cuantifica el error entre: como esta clasificando mi red los ejemplos y como deberia clasificarlos. Los terminos de la ecuacion corresponden a\n",
    "    1. theta: los pesos del modelo\n",
    "    1. n: es un learling rate que nos dice que tanto se deben actualizar los pesos, si hacemos la analogia del GD con al metodo de Newton, este vendria siendo una simplificacion de la matriz hessiana la cual al involucrar derivadas de segundo orden se vuelve costosisima de calcular.\n",
    "    \n",
    "1. si el valor de n es muy pequeño claramente la actualizacion iterativa de los pesos sera muy lenta y posiblemente necesitemos muchos ciclos para lograr llegar a un punto minimo, por otro lado si el valor de n es muy grande podriamos incluso nunca llegar al punto minimo y pasar \"saltando\" sobre el en numerosas oportunidades\n",
    "\n",
    "1. En el caso que la funcion de coste tenga minimos locales se pueden utilizar estrategias como \n",
    "    1. multiples inicializaciones diferentes para poder explorar el espacio, si obtenemos una convergencia consistente podriamos comenzar a pensar en que este valor podria ser candidato al minimo global (pero nunca estar seguros, a no ser que exploremos todo el maldito espacio) \n",
    "    1. tecnicas como adagrad vienen a intentar dar solucion a este tipo de problemas definiendo pasos diferentes para cada parametro y utilizando una heuristica que nos permita que los parametros que ya han explorado harto tengan un paso mas pequeño, obviamente cada tecnica tiene sus fortalezas y falencias. \n",
    "    \n",
    "1. Tipos SGD\n",
    "    1. SGD: consiste en realizar la actualizacion de pesos inmediatamente despues de pasar un ejemplo por la red, aqui todos los pesos provocaran actualizaciones de los pesos, es decir estamos actualizando los pesos despues del calculo de un solo gradiente obteniendo un entrenamiento mas ruidoso pero menos propenso a quedarse atrapado en minimos locales.\n",
    "    1. Batch: consiste en pasar todos los ejemplos por la red, calcular sus gradientes, acumularlos y despues de esto realizar la actualizacion correspondiente de los pesos, este estimador es mas propenso a quedarse atrapado en minimos locales, como opinion personal yo veo este como un metodo un \"poquito mas determinista\" ya que si yo tengo la misma inicializacion y hago batch, mi proximo punto siempre deberia ser el mismo ya que no tengo la opcion de realizar un shuffle, en que en caso del SGD podria permitir que avance de este (incluso si tiene la misma inicializacion) sea por diferentes caminos (y por algunos podria caer en minimo local y con otros no) \n",
    "    1. Mini-Batch: es un compromiso que trata de obtener lo mejor de ambos mundos, se presentan los N ejemplos en lotes de M < N (lo que no estoy seguro es si M % N debe ser = 0 o puede quedar un batch mas pequeño, o incluso, mas alla, si se pueden utilizar batches de diferentes tamaños (no se si tenga sentido))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "587e9f6133e759ffe44563205763b431",
     "grade": false,
     "grade_id": "cell-a6f78f1438811908",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Pregunta 2\n",
    "\n",
    "Una red neuronal para clasificación binaria con una capa oculta con función de activación  $\\phi(\\cdot)$ se describe matemáticamente como\n",
    "\n",
    "$$\n",
    "y = \\text{sgm} \\left( \\sum_{i=1}^{10} \\hat w_i \\cdot \\phi \\left( \\sum_{j=1}^2 w_{ij}\\cdot x_j + b_i \\right)  + \\hat b\\right),\n",
    "$$\n",
    "\n",
    "\n",
    "- (2pt) En este caso ¿Cuál es la dimensionalidad de la entrada? ¿Cuántas neuronas tiene la capa oculta? ¿Cuántos parámetros tiene la red en total? En el caso general ¿En qué influye la cantidad de neuronas y la cantidad de capas ocultas?\n",
    "- (2pt) ¿Qué ventaja otorga usar $\\phi(x)$ no lineal? Muestre que si $\\phi(x)=x$ entonces la red neuronal es equivalente a una neurona\n",
    "- (2pt) Si $\\phi(x) = \\max(0, x)$ (ReLU) ¿Qué ocurre si inicializamos con todos los parámetros iguales a cero? ¿Es posible entrenar la red con gradiente descendente? ¿Qué tipo de inicialización podría ser más conveniente?\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc514ac398efa7d776281e37b269fac8",
     "grade": true,
     "grade_id": "cell-97da95278d30d3a1",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. UNO\n",
    "    1. En este caso la dimensionalidad de la entrada deberia ser 2 \n",
    "    1. la capa oculta deberia tener 10 neuronas \n",
    "    1. la red en total deberia tener: \n",
    "        1. cada neurona de la capa oculta deberia tener 2 pesos + sesgo dando un total de 3 params por capa\n",
    "        1. eso debe ser multiplicado por las 10 capas ocultas dando un total de 30 parametros para esa primera parte\n",
    "        1. despues tenemos los pesos que utiliza la ultima capa mas el sesgo para finalmente activar la sigmoide que serian 10 pesos + 1 sesgo dando un total de 11 parametros\n",
    "    1. si mis calculos no me fallan la red deberia tenter en total 41 parametros.\n",
    "    1. la cantidad de neuronas y la cantidad de capas ocultas repercutira en la complejidad de los espacios que yo puedo formar para separar unas clases de otra, ya que cada neurona representa un hiperplano, mas neuronas me permiten combinar hiperplanos para formar cosas mas interesantes\n",
    "1. No es que otorge una ventaja, es que es necesario, puesto que como dice ahi si utilizaramos funciones de activaciones lineales toda la red podria ser reducida a una neurona en donde estos pesos se van sumando de manera lineal: tener 1*w1 + 2*w1 + 5*w1 es lo mismo que tener 8w1 (se que esta respuesta no \"demuestra\" lo que pide, disculpe)\n",
    "1. segun yo se quedaria pegada la red si la tratamos de entrenar con gradiente descendente y todos los parametros estan inicializados en cero, pero no estoy seguro. Si este es el caso podria ser mas conveniente una inicializacion aleatorea para evitar esto o alguna estrategia de transfer learning si existe una red previamente entrenada de dominio y aplicacion similar a mi problema (en esta tecnica se congelan las primeras capas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3690245655bf7ecf6166fced3ab7081",
     "grade": false,
     "grade_id": "cell-3775e515da503d59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "\n",
    "## Pregunta 3\n",
    "\n",
    "Sea una imagen de un sólo canal definida como\n",
    "\n",
    "```python\n",
    "I = [[2, 0, 0, 1, 0],\n",
    "     [0, 1, 1, 2, 0],\n",
    "     [3, 1, 0, 1, 0], \n",
    "     [0, 2, 0, 1, 1], \n",
    "     [1, 1, 3, 1, 2]]\n",
    "```\n",
    "\n",
    "y una capa convolucional con dos filtros definidos como\n",
    "\n",
    "```python\n",
    "K1 = [[0, 1, 0], \n",
    "      [1, 1, 1], \n",
    "      [0, 1, 0]]\n",
    "\n",
    "K2 = [[-1, 1, 0], \n",
    "      [-1, 1, 0], \n",
    "      [-1, 1, 0]]\n",
    "\n",
    "```\n",
    "\n",
    "Esta capa convolucional no tiene sesgos\n",
    "\n",
    "- (4pt) Asumiendo un stride de 2 y que se aplica *zero-padding* de 1 pixel en la entrada: Obtenga los *features maps* de esta capa convolucional\n",
    "- (2pt) Aplique la función de activación ReLU a los *feature maps*, luego realice una operación de max-pooling de tamaño $2\\times2$ con stride de $1$ (sin padding) y muestre el resultado. Indique desde que posición/es en la entrada se origina la información que queda luego de aplicar max-pooling\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36e435e6cb6287a57a5b9434d7d8b048",
     "grade": true,
     "grade_id": "cell-95c619d17850748b",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "#### Feature maps\n",
    "\n",
    "```python\n",
    "I + padding= [[0, 0, 0, 0, 0, 0, 0],\n",
    "              [0, 2, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 1, 1, 2, 0, 0],\n",
    "              [0, 3, 1, 0, 1, 0, 0], \n",
    "              [0, 0, 2, 0, 1, 1, 0], \n",
    "              [0, 1, 1, 3, 1, 2, 0],\n",
    "              [0, 0, 0, 0, 0, 0, 0]]\n",
    "\n",
    "FM1 = [[2, 2, 1],\n",
    "       [4, 3, 2],\n",
    "       [2, 5, 4]]\n",
    "\n",
    "FM2 = [[2, 0, -3],\n",
    "       [3, -3, -3],\n",
    "       [1, 0, 1]]\n",
    "```\n",
    "\n",
    "####  FM after Relu\n",
    "\n",
    "```python\n",
    "\n",
    "FM1 = [[2, 2, 1],\n",
    "       [4, 3, 2],\n",
    "       [2, 5, 4]]\n",
    "\n",
    "FM2 = [[2, 0, 0],\n",
    "       [3, 0, 0],\n",
    "       [1, 0, 1]]\n",
    "```\n",
    "\n",
    "no entiendo a que se refiere con eso:  \"Indique desde que posición/es en la entrada se origina la información que queda luego de aplicar max-pooling\" pero intentare responder lo que entiendo\n",
    "\n",
    "####  FM after max pooling\n",
    "\n",
    "```python\n",
    "\n",
    "FM1_max_p = [[4, 3],\n",
    "             [5, 5]]\n",
    "\n",
    "y vienen de aqui \n",
    "\n",
    "FM1_max_p[0,0]= FM1[1,0]\n",
    "FM1_max_p[0,1]= FM1[1,1]\n",
    "FM1_max_p[1,0]= FM1[2,1]\n",
    "FM1_max_p[1,1]= FM1[2,1]\n",
    "```\n",
    "\n",
    "```python\n",
    "FM2_max_p = [[3, 0],\n",
    "             [3, 1]]\n",
    "\n",
    "y vienen de aqui \n",
    "\n",
    "FM1_max_p[0,0]= FM1[1,0]\n",
    "FM1_max_p[0,1]= FM1[1,1] #no estoy seguro de cual viene\n",
    "FM1_max_p[1,0]= FM1[1,0]\n",
    "FM1_max_p[1,1]= FM1[2,2]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6dc60b1a57ca8aac08e581846b37243b",
     "grade": false,
     "grade_id": "cell-393f29de61cf28b4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "***\n",
    "## Pregunta 4\n",
    "\n",
    "Para entrenar un modelo supervisado usted particiona sus datos en conjuntos de **Entrenamiento, Validación y Prueba**\n",
    "\n",
    "Al finalizar el entrenamiento usted mide la función de costo o *loss function* total en cada conjunto obteniendo $L_E$, $L_V$ y $L_P$, respectivamente\n",
    "\n",
    "\n",
    "- (2pt) Suponga que $L_E \\ll L_V$ ¿Qué puede teorizar sobre la capacidad/complejidad de su modelo? ¿Qué puede teorizar sobre la cantidad de datos de entrenamiento que se usaron? \n",
    "- (4pt) Escoja dos de los siguientes métodos, descríbalos en detalle explicando cómo ayudan a combatir el sobreajuste del modelo\n",
    "    - Early Stopping\n",
    "    - Aumentación de datos \n",
    "    - Decaimiento de pesos\n",
    "    - Dropout\n",
    "***    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98a037085b7ff86fff2a95b38a6cde5b",
     "grade": true,
     "grade_id": "cell-6c62b9cc52d5eadd",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "1. acerca de la complejidad de la capacidad del modelo, se podrian aventurar conclusiones del estilo, el modelo se esta sobre ajustando y es bueno (en realidad es mejor no sabemos si es bueno, solo son comparaciones relativas) para clasificar los ejemplos del data set de entrenamiento, pero no reconocer los del data set de validacion, con respecto a la cantidad de datos utilizados, podriamos decir que es probable que no se esten utilizando los datos suficientes para obtener un modelo que sea capaz de generalizar para otros casos fuera de los que se utilizan para entrenarlo\n",
    "1. \n",
    "    1. Early stopping: consiste en poner una condicion de parada la cual tipicamente consiste en detener el entrenamiento cuando mi error de valdiacion comienza a aumentar despues de un par de epocas seguidas (paciencia) \n",
    "    1. Aumentacion de datos: consiste en aumentar mi data set de manera artificial mediante herramientas como transformaciones que hacen cosas como:  escalar, rotar y desplazar la imagen original (en este caso de CNN) o incluso agregar diferentes tipos de ruidos para lograr que nuestro modelo se vuelva invariante a estos cambios (ruido o desplazamiento por ejemplo). de esta manera se busca que el modelo sea capaz de generalizar de una mejor manera al entregarle muchisimos mas ejemplos para su entrenamiento: es importante mencionar que las transofmraciones o modificaciones realizadas deben tener sentido en el dominio del problema y que solo deben ser aplicadas en el data set de entrenamiento para no sesgar el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
